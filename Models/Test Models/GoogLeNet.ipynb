{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6U//ygWmSVrBcVJ4WKgih"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCL112EbFLik","outputId":"56584ee5-04f3-42c9-dd00-777e16a86e56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opendatasets in /usr/local/lib/python3.9/dist-packages (0.1.22)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from opendatasets) (8.1.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from opendatasets) (4.65.0)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (from opendatasets) (1.5.13)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2022.12.7)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.16.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.26.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.27.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (8.0.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (3.4)\n","Skipping, found downloaded files in \"./fruit-and-vegetable-image-recognition\" (use force=True to force download)\n","Found 3115 files belonging to 36 classes.\n","Found 351 files belonging to 36 classes.\n","Epoch 1/20\n","49/49 [==============================] - 988s 20s/step - loss: 28.7532 - accuracy: 0.0356 - val_loss: 5.0849 - val_accuracy: 0.0399\n","Epoch 2/20\n","49/49 [==============================] - 978s 20s/step - loss: 5.1185 - accuracy: 0.0475 - val_loss: 4.6431 - val_accuracy: 0.0627\n","Epoch 3/20\n","49/49 [==============================] - 956s 19s/step - loss: 3.7847 - accuracy: 0.0700 - val_loss: 3.4287 - val_accuracy: 0.0826\n","Epoch 4/20\n","49/49 [==============================] - 947s 19s/step - loss: 3.5558 - accuracy: 0.0838 - val_loss: 3.2841 - val_accuracy: 0.1197\n","Epoch 5/20\n","49/49 [==============================] - 938s 19s/step - loss: 3.4376 - accuracy: 0.0982 - val_loss: 3.2859 - val_accuracy: 0.1282\n","Epoch 6/20\n","49/49 [==============================] - 946s 19s/step - loss: 3.3256 - accuracy: 0.1194 - val_loss: 3.1483 - val_accuracy: 0.1567\n","Epoch 7/20\n","49/49 [==============================] - 941s 19s/step - loss: 3.2654 - accuracy: 0.1239 - val_loss: 3.0578 - val_accuracy: 0.1624\n","Epoch 8/20\n","49/49 [==============================] - 943s 19s/step - loss: 3.1940 - accuracy: 0.1525 - val_loss: 2.9635 - val_accuracy: 0.1738\n","Epoch 9/20\n","49/49 [==============================] - 897s 18s/step - loss: 3.1013 - accuracy: 0.1557 - val_loss: 2.8312 - val_accuracy: 0.2137\n","Epoch 10/20\n","49/49 [==============================] - 942s 19s/step - loss: 3.0318 - accuracy: 0.1807 - val_loss: 2.8062 - val_accuracy: 0.2194\n","Epoch 11/20\n","49/49 [==============================] - 948s 19s/step - loss: 3.0411 - accuracy: 0.1692 - val_loss: 2.5323 - val_accuracy: 0.2906\n","Epoch 12/20\n","49/49 [==============================] - 943s 19s/step - loss: 2.9450 - accuracy: 0.1888 - val_loss: 2.7699 - val_accuracy: 0.2365\n","Epoch 13/20\n","49/49 [==============================] - 896s 18s/step - loss: 2.9112 - accuracy: 0.1974 - val_loss: 2.5802 - val_accuracy: 0.2621\n","Epoch 14/20\n","49/49 [==============================] - 945s 19s/step - loss: 2.7921 - accuracy: 0.2231 - val_loss: 2.5707 - val_accuracy: 0.2821\n","Epoch 15/20\n","49/49 [==============================] - 898s 18s/step - loss: 2.8060 - accuracy: 0.2321 - val_loss: 2.6376 - val_accuracy: 0.3134\n","Epoch 16/20\n","49/49 [==============================] - 957s 19s/step - loss: 2.7619 - accuracy: 0.2376 - val_loss: 2.4250 - val_accuracy: 0.3362\n","Epoch 17/20\n","49/49 [==============================] - 907s 18s/step - loss: 2.7514 - accuracy: 0.2331 - val_loss: 2.6237 - val_accuracy: 0.2821\n","Epoch 18/20\n","49/49 [==============================] - 965s 20s/step - loss: 2.6637 - accuracy: 0.2526 - val_loss: 2.0665 - val_accuracy: 0.4444\n","Epoch 19/20\n","49/49 [==============================] - 965s 19s/step - loss: 2.6380 - accuracy: 0.2726 - val_loss: 2.1724 - val_accuracy: 0.3789\n","Epoch 20/20\n","49/49 [==============================] - 969s 20s/step - loss: 2.5833 - accuracy: 0.2854 - val_loss: 2.0447 - val_accuracy: 0.4131\n","49/49 [==============================] - 810s 16s/step - loss: 2.4692 - accuracy: 0.3002\n","Training Accuracy: 0.3001605272293091\n","Epoch 1/20\n","49/49 [==============================] - 953s 19s/step - loss: 2.7656 - accuracy: 0.2713 - val_loss: 1.9993 - val_accuracy: 0.4444\n","Epoch 2/20\n"," 8/49 [===>..........................] - ETA: 11:20 - loss: 2.5160 - accuracy: 0.2910"]}],"source":["!pip install opendatasets\n","import numpy as np\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.layers import GlobalAveragePooling2D, Dense\n","import matplotlib.pyplot as plt\n","import opendatasets as od\n","od.download(\"https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition\")\n","training_set = tf.keras.utils.image_dataset_from_directory(\n","    '/content/fruit-and-vegetable-image-recognition/train',\n","    labels=\"inferred\",\n","    label_mode=\"categorical\",\n","    class_names=None,\n","    color_mode=\"rgb\",\n","    batch_size=64,\n","    image_size=(299, 299), # for InceptionV3 model\n","    shuffle=True,\n","    seed=None,\n","    validation_split=None,\n","    subset=None,\n","    interpolation=\"bilinear\",\n","    follow_links=False,\n","    crop_to_aspect_ratio=False\n",")\n","test_set = tf.keras.utils.image_dataset_from_directory(\n","    '/content/fruit-and-vegetable-image-recognition/validation',\n","    labels=\"inferred\",\n","    label_mode=\"categorical\",\n","    class_names=None,\n","    color_mode=\"rgb\",\n","    batch_size=64,\n","    image_size=(299, 299), # for InceptionV3 model\n","    shuffle=True,\n","    seed=None,\n","    validation_split=None,\n","    subset=None,\n","    interpolation=\"bilinear\",\n","    follow_links=False,\n","    crop_to_aspect_ratio=False\n",")\n","\n","base_model = InceptionV3(input_shape=(299, 299, 3), include_top=False, weights='imagenet')\n","\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(units=512,activation='relu')(x)\n","x = Dense(units=256,activation='relu')(x)\n","predictions = Dense(units=36, activation='softmax')(x)\n","cnn = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n","\n","cnn.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","training_history = cnn.fit(x=training_set,validation_data=test_set,epochs=20)\n","training_loss, training_accuracy = cnn.evaluate(training_set)\n","print('Training Accuracy:', training_accuracy)\n","\n","cnn.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n","training_history = cnn.fit(x=training_set,validation_data=test_set,epochs=20)\n","training_loss, training_accuracy = cnn.evaluate(training_set)\n","print('Training Accuracy:', training_accuracy)\n","\n","cnn.save(\"trained_model.h5\")\n"]}]}